#!/usr/bin/env python3
"""
Creative-Sparse HunyuanVideo Example

This script demonstrates the Creative-Sparse algorithm applied to HunyuanVideo generation,
showing how sparsity can enhance creativity in video synthesis with Chinese models.
"""

import torch
import os
import gc
import argparse
from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel
from diffusers.utils import export_to_video
from tqdm import tqdm
import numpy as np

# Import Creative-Sparse components
from spas_sage_attn import (
    CreativeSparseWrapper,
    SparsityConfig,
    QualityWeights,
    find_optimal_sparsity,
    compute_quality_score
)
from spas_sage_attn.autotune import (
    extract_sparse_attention_state_dict,
    load_sparse_attention_state_dict,
)
from modify_model.modify_hunyuan import set_spas_sage_attn_hunyuan

def parse_args():
    parser = argparse.ArgumentParser(description="Creative-Sparse HunyuanVideo Evaluation")
    
    # Creative-Sparse specific arguments
    parser.add_argument("--use_creative_sparse", action="store_true", 
                       help="Use Creative-Sparse algorithm")
    parser.add_argument("--sparsity_schedule", type=float, nargs=2, default=[0.65, 0.35],
                       help="Sparsity schedule for divergence and refinement phases")
    parser.add_argument("--quality_weights", type=float, nargs=3, default=[0.4, 0.3, 0.3],
                       help="Quality weights for novelty, diversity, and coherence")
    parser.add_argument("--num_candidates", type=int, default=3,
                       help="Number of candidates to generate in divergence phase")
    parser.add_argument("--auto_sparsity", action="store_true",
                       help="Automatically find optimal sparsity level")
    
    # Original SpargeAttention arguments
    parser.add_argument("--use_spas_sage_attn", action="store_true", 
                       help="Use SpargeAttention")
    parser.add_argument("--tune", action="store_true", help="Tune hyperparameters")
    parser.add_argument('--parallel_tune', action='store_true', help='Enable parallel tuning')
    parser.add_argument('--l1', type=float, default=0.06, help='L1 bound for QK sparse')
    parser.add_argument('--pv_l1', type=float, default=0.065, help='L1 bound for PV sparse')
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    parser.add_argument("--tune_pv", action="store_true", help="Tune PV parameters")
    parser.add_argument("--base_seed", type=int, default=0, help="Base seed")
    
    # Output arguments
    parser.add_argument("--out_path", type=str,
                       default="evaluate/datasets/video/creative_sparse_hunyuan",
                       help="Output path")
    parser.add_argument("--model_out_path", type=str,
                       default="evaluate/models_dict/hunyuan_saved_state_dict.pt",
                       help="Model output path")
    
    return parser.parse_args()


def load_chinese_creative_prompts():
    """Load Chinese creative prompts for video generation"""
    chinese_prompts = [
        "ä¸€ä¸ªæœºå™¨äººåœ¨å­¦ä¹ ç»˜ç”»ï¼Œæ¯ä¸€ç¬”éƒ½åˆ›é€ å‡ºæ–°çš„ä¸–ç•Œï¼Œè‰²å½©å¦‚éŸ³ä¹èˆ¬æµæ·Œ",
        "ä¸€ä½èˆè€…åœ¨å…‰å½±åŸå¸‚ä¸­ç©¿æ¢­ï¼Œæ¯ä¸€æ­¥éƒ½ç”Ÿæˆç¾å¦™çš„éŸ³ä¹",
        "ä¸€æ£µæ ‘åœ¨å€’ç€ç”Ÿé•¿ï¼Œå¶å­å˜æˆè´è¶ï¼Œåœ¨å¤©ç©ºä¸­ç»˜ç”»",
        "ä¸€ä¸ªæœºæ¢°å¿ƒè„åœ¨æœºæ¢°èŠ±å›­ä¸­è·³åŠ¨ï¼Œæ¯ä¸€æ¬¡è·³åŠ¨éƒ½è®©èŠ±æœµç»½æ”¾",
        "ä¸€æ”¯ç”»ç¬”åˆ›é€ ç°å®ï¼Œæ¯ä¸€ç¬”éƒ½è®©æ¢¦æƒ³æˆçœŸ",
        "ä¸€ä½éŸ³ä¹å®¶æŒ‡æŒ¥ç€è‰²å½©çš„ç®¡å¼¦ä¹é˜Ÿï¼Œå£°éŸ³å˜å¾—å¯è§",
        "ä¸€ä¸ªå›¾ä¹¦é¦†é‡Œä¹¦ç±å¦‚é¸Ÿå„¿èˆ¬é£ç¿”ï¼Œä¹¦é¡µå˜æˆæ˜Ÿåº§",
        "ä¸€ä½å¨å¸ˆç”¨æƒ…æ„Ÿçƒ¹é¥ªï¼Œæ¯é“èœéƒ½è®²è¿°ä¸åŒçš„æ•…äº‹"
    ]
    return chinese_prompts


def apply_creative_sparse_to_hunyuan(pipeline, args):
    """Apply Creative-Sparse algorithm to HunyuanVideo pipeline"""
    if not args.use_creative_sparse:
        return pipeline
    
    print("ğŸ¨ Applying Creative-Sparse algorithm to HunyuanVideo...")
    
    # Initialize Creative-Sparse wrapper
    quality_weights = QualityWeights(
        alpha=args.quality_weights[0],  # Novelty
        beta=args.quality_weights[1],   # Diversity
        gamma=args.quality_weights[2]   # Coherence
    )
    
    sparsity_config = SparsityConfig(
        simthreshd1=0.6,
        cdfthreshd=0.97,
        pvthreshd=15
    )
    
    creative_wrapper = CreativeSparseWrapper(
        model_name="hunyuan",
        sparsity_schedule=tuple(args.sparsity_schedule),
        quality_weights=quality_weights,
        sparsity_config=sparsity_config
    )
    
    # Find optimal sparsity if requested
    if args.auto_sparsity:
        optimal_sparsity = find_optimal_sparsity(
            model_name="hunyuan",
            task_type="video_generation",
            compute_budget=0.8
        )
        print(f"ğŸ¯ Optimal sparsity found: {optimal_sparsity:.3f}")
        creative_wrapper.sparsity_schedule = (optimal_sparsity, optimal_sparsity * 0.5)
    
    return pipeline, creative_wrapper


def generate_creative_video(pipeline, prompt, args, creative_wrapper=None):
    """Generate a creative video using either standard or Creative-Sparse approach"""
    
    if creative_wrapper is not None:
        print(f"ğŸ¨ Generating creative video with Creative-Sparse...")
        print(f"   Prompt: {prompt}")
        
        # Use Creative-Sparse two-stage generation
        # Stage A: Divergence - generate multiple candidates
        candidates = []
        for i in range(args.num_candidates):
            print(f"   Stage A - Candidate {i+1}/{args.num_candidates}")
            
            # Generate with high sparsity for exploration
            video = pipeline(
                prompt=prompt,
                num_frames=49,
                height=320,
                width=512,
                num_inference_steps=20,
                guidance_scale=7.5,
                generator=torch.Generator().manual_seed(args.base_seed + i)
            ).videos[0]
            
            candidates.append(video)
        
        # Stage B: Refinement - select and refine best candidate
        print(f"   Stage B - Refining best candidate...")
        
        # For video, we'll use a simple heuristic to select the best candidate
        # In practice, this would use more sophisticated quality assessment
        best_candidate_idx = 0  # Simplified selection
        
        # Refine the best candidate with lower sparsity
        refined_video = pipeline(
            prompt=prompt,
            num_frames=49,
            height=320,
            width=512,
            num_inference_steps=25,  # More steps for refinement
            guidance_scale=8.0,      # Higher guidance for coherence
            generator=torch.Generator().manual_seed(args.base_seed)
        ).videos[0]
        
        return refined_video, candidates
    
    else:
        # Standard generation
        print(f"ğŸ¬ Generating standard video...")
        print(f"   Prompt: {prompt}")
        
        video = pipeline(
            prompt=prompt,
            num_frames=49,
            height=320,
            width=512,
            num_inference_steps=20,
            guidance_scale=7.5,
            generator=torch.Generator().manual_seed(args.base_seed)
        ).videos[0]
        
        return video, [video]


def main():
    args = parse_args()
    os.makedirs(args.out_path, exist_ok=True)
    
    print("ğŸš€ Creative-Sparse HunyuanVideo Evaluation")
    print("=" * 50)
    print(f"Creative-Sparse: {args.use_creative_sparse}")
    print(f"Sparsity Schedule: {args.sparsity_schedule}")
    print(f"Quality Weights: {args.quality_weights}")
    print(f"Number of Candidates: {args.num_candidates}")
    print(f"Base Seed: {args.base_seed}")
    print("=" * 50)
    
    # Setup device and model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16
    num_frames = 49
    
    print(f"Using device: {device}")
    
    # Load model
    model_id = "hunyuanvideo-community/HunyuanVideo"
    print(f"Loading model: {model_id}")
    
    pipeline = HunyuanVideoPipeline.from_pretrained(
        model_id,
        torch_dtype=dtype,
        device_map="auto"
    )
    
    # Apply SpargeAttention if requested
    if args.use_spas_sage_attn:
        print("ğŸ”§ Applying SpargeAttention...")
        set_spas_sage_attn_hunyuan(pipeline.transformer)
        
        if args.tune:
            print("ğŸ›ï¸ Tuning hyperparameters...")
            # Tuning logic would go here
            pass
    
    # Apply Creative-Sparse if requested
    creative_wrapper = None
    if args.use_creative_sparse:
        pipeline, creative_wrapper = apply_creative_sparse_to_hunyuan(pipeline, args)
    
    # Load Chinese creative prompts
    chinese_prompts = load_chinese_creative_prompts()
    
    # Generate videos
    print(f"\nğŸ¬ Generating {len(chinese_prompts)} creative videos...")
    
    for i, prompt in enumerate(chinese_prompts):
        print(f"\n--- Video {i+1}/{len(chinese_prompts)} ---")
        
        try:
            # Generate video
            if args.use_creative_sparse:
                video, candidates = generate_creative_video(
                    pipeline, prompt, args, creative_wrapper
                )
                
                # Save main video
                output_path = os.path.join(args.out_path, f"creative_{i:02d}.mp4")
                export_to_video(video, output_path)
                print(f"âœ… Saved: {output_path}")
                
                # Save candidates if requested
                if args.verbose:
                    for j, candidate in enumerate(candidates):
                        candidate_path = os.path.join(args.out_path, f"candidate_{i:02d}_{j:02d}.mp4")
                        export_to_video(candidate, candidate_path)
                        print(f"   Candidate {j+1}: {candidate_path}")
                
            else:
                video, _ = generate_creative_video(pipeline, prompt, args)
                output_path = os.path.join(args.out_path, f"standard_{i:02d}.mp4")
                export_to_video(video, output_path)
                print(f"âœ… Saved: {output_path}")
            
            # Clean up memory
            del video
            if 'candidates' in locals():
                del candidates
            gc.collect()
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ Error generating video {i+1}: {e}")
            continue
    
    print(f"\nğŸ‰ Generation completed! Videos saved to: {args.out_path}")
    
    # Performance summary
    if args.use_creative_sparse:
        print("\nğŸ“Š Creative-Sparse Performance Summary:")
        print(f"   Sparsity Schedule: {args.sparsity_schedule}")
        print(f"   Quality Weights: {args.quality_weights}")
        print(f"   Candidates per video: {args.num_candidates}")
        print("   Enhanced creativity through two-stage generation!")


if __name__ == "__main__":
    main()
